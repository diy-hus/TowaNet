{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "#Importing libraries and loading dataset\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "a0F_MNr1uiwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/df_train_watering.csv\")"
      ],
      "metadata": {
        "id": "9sczqUjqaod3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_columns_df = df.columns.tolist()\n",
        "dataset_columns_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAMj7cDQbVNK",
        "outputId": "b0bbcde2-90d6-49c8-fa52-bba11364b6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hour',\n",
              " 'Minute',\n",
              " 'Light',\n",
              " 'Temp',\n",
              " 'Humid',\n",
              " 'Soil',\n",
              " 'Light2',\n",
              " 'Temp2',\n",
              " 'Humid2',\n",
              " 'Soil2',\n",
              " 'Light3',\n",
              " 'Temp3',\n",
              " 'Humid3',\n",
              " 'Soil3',\n",
              " 'Light4',\n",
              " 'Temp4',\n",
              " 'Humid4',\n",
              " 'Soil4']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Encode non categorical values\n",
        "values_org = df.values\n",
        "\n",
        "hours = values_org[:, 0]\n",
        "minutes = values_org[:, 1]\n",
        "combined_time = hours + minutes / 60"
      ],
      "metadata": {
        "id": "2Rss3QM6Tf2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = np.column_stack((combined_time, values_org[:, 2:]))\n",
        "values, values.shape"
      ],
      "metadata": {
        "id": "qFOe0l1mo2bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalising data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_dataset = scaler.fit_transform(values)"
      ],
      "metadata": {
        "id": "YEl-Sf0iTkGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"/content/df_test_watering.csv\")\n",
        "\n",
        "values_org_test = df_test.values\n",
        "\n",
        "hours = values_org_test[:, 0]\n",
        "minutes = values_org_test[:, 1]\n",
        "combined_time = hours + minutes / 60\n",
        "\n",
        "data_of_test = values_org_test[:, 2:]\n",
        "data_of_test = data_of_test.reshape((-1,1,4))\n",
        "data_of_test = np.repeat(data_of_test, 4, axis=1)\n",
        "data_of_test = data_of_test.reshape((-1,16))\n",
        "\n",
        "values_test = np.column_stack((combined_time, data_of_test))"
      ],
      "metadata": {
        "id": "_v0TgkohgII7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_scaled_dataset = scaler.transform(values_test)"
      ],
      "metadata": {
        "id": "Mb7tT8vcgB7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self, num_vertices):\n",
        "        self.num_vertices = num_vertices\n",
        "        self.adj_matrix = [[0 for _ in range(num_vertices)] for _ in range(num_vertices)]\n",
        "\n",
        "    def add_edge(self, src, dest, weight=1):\n",
        "        if 0 <= src < self.num_vertices and 0 <= dest < self.num_vertices:\n",
        "            self.adj_matrix[src][dest] = weight\n",
        "            self.adj_matrix[dest][src] = weight  # Assuming the graph is undirected\n",
        "\n",
        "    def remove_edge(self, src, dest):\n",
        "        if 0 <= src < self.num_vertices and 0 <= dest < self.num_vertices:\n",
        "            self.adj_matrix[src][dest] = 0\n",
        "            self.adj_matrix[dest][src] = 0  # Assuming the graph is undirected\n",
        "\n",
        "    def get_weight(self, src, dest):\n",
        "        if 0 <= src < self.num_vertices and 0 <= dest < self.num_vertices:\n",
        "            return self.adj_matrix[src][dest]\n",
        "        return None\n",
        "\n",
        "    def get_adj_matrix(self):\n",
        "        return np.array(self.adj_matrix, dtype=np.float32)\n",
        "\n",
        "\n",
        "def corr_sensor(sen1, sen2):\n",
        "    correlation_12, _ = kendalltau(sen1, sen2)\n",
        "    return correlation_12\n",
        "\n",
        "def corr_station(station1, station2):\n",
        "    ligh_corr = corr_sensor(station1[:,0], station2[:,0])\n",
        "    temp_corr = corr_sensor(station1[:,1], station2[:,1])\n",
        "\n",
        "    humd_corr = corr_sensor(station1[:,2], station2[:,2])\n",
        "    soil_corr = corr_sensor(station1[:,3], station2[:,3])\n",
        "\n",
        "    corr_result = [ligh_corr, temp_corr, humd_corr, soil_corr]\n",
        "\n",
        "    return sum(corr_result)/len(corr_result)\n",
        "\n",
        "from itertools import combinations\n",
        "# List chứa 4 phần tử\n",
        "elements = [1, 2, 3, 4]\n",
        "\n",
        "# Tìm tổ hợp chập 2 của các phần tử\n",
        "combinations_2 = list(combinations(elements, 2))\n",
        "\n",
        "# In kết quả\n",
        "print(combinations_2)\n",
        "\n",
        "\n",
        "data_sensor1 = scaled_dataset[:,-4:]\n",
        "data_sensor2 = scaled_dataset[:,-8:-4]\n",
        "data_sensor3 = scaled_dataset[:,-12:-8]\n",
        "data_sensor4 = scaled_dataset[:,-16:-12]\n",
        "\n",
        "data_sensors = [data_sensor1, data_sensor2, data_sensor3, data_sensor4]\n",
        "\n",
        "set_of_sensors = [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
        "\n",
        "num_vertices = 4\n",
        "graph = Graph(num_vertices)\n",
        "\n",
        "for set_of_sensor in set_of_sensors:\n",
        "    index1 ,index2 = set_of_sensor[0]-1, set_of_sensor[1]-1\n",
        "\n",
        "    corr_station_result = corr_station(data_sensors[index1], data_sensors[index2])\n",
        "\n",
        "    graph.add_edge(index1, index2, corr_station_result)\n",
        "\n",
        "adj_matrix_np = graph.get_adj_matrix()"
      ],
      "metadata": {
        "id": "GlFZT6X59ghx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_supervised(train):\n",
        "  window_size = 6\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range(window_size, len(train)):\n",
        "    X.append(train[i-window_size:i,:])\n",
        "    Y.append(train[i,1:])\n",
        "\n",
        "  return X,Y"
      ],
      "metadata": {
        "id": "ZDmgpYtLUYSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = to_supervised(scaled_dataset)\n",
        "X_test, Y_test = to_supervised(test_scaled_dataset)\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test)\n",
        "print('X_train' ,X_train.shape)\n",
        "print('Y_train' ,Y_train.shape)\n",
        "print('X_test' ,X_test.shape)\n",
        "print('Y_test' ,Y_test.shape)"
      ],
      "metadata": {
        "id": "k1r7RJjIUooQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "        self.input_dim = input_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False, dot_att=False):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        if mask is not None:\n",
        "            mask = expand_mask(mask)\n",
        "        qkv = self.qkv_proj(x)\n",
        "        # print(x.shape)\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        # print(v.shape)\n",
        "        # Determine value outputs\n",
        "        if dot_att:\n",
        "            # x_other =\n",
        "            x_other = x.reshape(batch_size, seq_length, self.num_heads, -1)\n",
        "            x_other = x_other.permute(0, 2, 1, 3)\n",
        "            values, attention = scaled_dot_product(q, k, x_other, mask=mask)\n",
        "            values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "            values = values.reshape(batch_size, seq_length, -1)\n",
        "            if return_attention:\n",
        "                return values, attention\n",
        "            else:\n",
        "                return values\n",
        "        else:\n",
        "            values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "            values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "            values = values.reshape(batch_size, seq_length, -1)\n",
        "            o = self.o_proj(values)\n",
        "\n",
        "            if return_attention:\n",
        "                return o, attention\n",
        "            else:\n",
        "                return o\n",
        "\n",
        "class GraphConvolutionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolutionLayer, self).__init__()\n",
        "        self.projection = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x, adj_matrix):\n",
        "        # Num neighbours = number of incoming edges\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.projection(x)\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gc1 = GraphConvolutionLayer((input_dim-1)//4+1, hidden_dim)\n",
        "        self.gc2 = GraphConvolutionLayer(hidden_dim, output_dim)\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, x_seq, adj_matrix):\n",
        "\n",
        "        x_seq = self._preprocessing(x_seq)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(x_seq.shape[1]):\n",
        "            x = x_seq[:, t]\n",
        "            x = F.relu(self.gc1(x, adj_matrix))\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            x = F.relu(self.gc2(x, adj_matrix))\n",
        "            outputs.append(x)\n",
        "\n",
        "        output_seq = torch.stack(outputs, dim=1)\n",
        "        return output_seq.view(*output_seq.shape[:2],-1)\n",
        "\n",
        "    def _preprocessing(self, x):\n",
        "        b, t, c = x.shape\n",
        "        x_time = x[:, :, :1].view(b, t, 1, 1)\n",
        "        x_feat = x[:, :, 1:].view(b, t, 4, 4)\n",
        "\n",
        "        # Repeat x_time along axis 2 to match the shape of x_feat\n",
        "        x_time_repeated = x_time.repeat(1, 1, x_feat.shape[2], 1)\n",
        "\n",
        "        # Concatenate x_time and x_feat along axis 3\n",
        "        x_concatenated = torch.cat((x_time_repeated, x_feat), dim=3)\n",
        "        # print(x_concatenated.shape)\n",
        "        return x_concatenated\n",
        "\n",
        "\n",
        "\n",
        "class SqueezeExcitationBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=16):\n",
        "        super(SqueezeExcitationBlock, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "        # Squeeze operation to obtain global channel-wise information\n",
        "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Excitation operation to learn channel-wise importance\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_channels // reduction_ratio, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, _= x.size()\n",
        "\n",
        "        # Squeeze: Global information across channels\n",
        "        squeezed = self.squeeze(x)\n",
        "        squeezed = squeezed.view(batch_size, channels)\n",
        "\n",
        "        # Excitation: Learn channel-wise importance\n",
        "        excitations = self.excitation(squeezed)\n",
        "        excitations = excitations.view(batch_size, channels, 1)\n",
        "\n",
        "        # Scale the input feature map\n",
        "        scaled_x = x * excitations\n",
        "\n",
        "        return scaled_x\n",
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_batchnorm=True, use_se_block=True, use_residual=True, dropout_prob=0.2):\n",
        "        super(CNNBlock, self).__init__()\n",
        "\n",
        "        # First Convolutional layer\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "        # Second Convolutional layer\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "        # Batch normalization (optional)\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        if use_batchnorm:\n",
        "            self.batchnorm1 = nn.BatchNorm1d(out_channels)\n",
        "            self.batchnorm2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.ReLU(inplace=True)  # ReLU is commonly used, but other activations can be used too\n",
        "\n",
        "        self.use_se_block = use_se_block\n",
        "        if use_se_block:\n",
        "            self.se_block = SqueezeExcitationBlock(out_channels)\n",
        "\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # Apply batch normalization if enabled\n",
        "        if self.use_batchnorm:\n",
        "            x = self.batchnorm1(x)\n",
        "\n",
        "        x = self.activation(x)\n",
        "\n",
        "\n",
        "\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # Apply batch normalization if enabled\n",
        "        if self.use_batchnorm:\n",
        "            x = self.batchnorm2(x)\n",
        "\n",
        "        x = self.activation(x)\n",
        "\n",
        "        if self.dropout_prob > 0:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        if self.use_se_block:\n",
        "            x = self.se_block(x)\n",
        "\n",
        "        if self.use_residual:\n",
        "            x = x + identity\n",
        "\n",
        "        return x\n",
        "\n",
        "class GCNCNNSELSTMATTModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, drop_out):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gcn = GCN(input_size, hidden_size, hidden_size)\n",
        "        self.cnn = CNNBlock(hidden_size*4, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=drop_out, batch_first=True)\n",
        "\n",
        "\n",
        "        num_heads = 8\n",
        "        self.multihead_attn = MultiheadAttention(hidden_size, hidden_size, num_heads)\n",
        "\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_size*6, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 16)\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        x = self.gcn(x, adjs)\n",
        "\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(0,2,1)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.multihead_attn(out)\n",
        "\n",
        "        out = self.fc(torch.flatten(out, 1))\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(torch.relu(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom Dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y, adj_matrix):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.adj_matrix = adj_matrix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], torch.from_numpy(self.adj_matrix)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sb4mUrfPtTHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating an instance of the LSTMModel\n",
        "input_size = X_train.shape[2]\n",
        "hidden_size = 64\n",
        "num_layers = 3\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNNSELSTMATTModel(input_size, hidden_size, num_layers, drop_out=0.2)\n",
        "\n",
        "# Defining the loss function and optimizer\n",
        "mse_loss_fun = nn.MSELoss()\n",
        "mae_loss = nn.L1Loss()\n",
        "def criterion(x,y):\n",
        "    # return mse_loss_fun\n",
        "    loss_value = mse_loss_fun(x,y)\n",
        "    return loss_value\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Converting the data to PyTorch tensors\n",
        "X_train_tensor = torch.from_numpy(X_train).float()\n",
        "y_train_tensor = torch.from_numpy(Y_train).float()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
        "y_test_tensor = torch.from_numpy(Y_test).float().to(device)\n",
        "adj_matrix_test = torch.from_numpy(adj_matrix_np).view(1,*adj_matrix_np.shape)\n",
        "adj_matrix_test = adj_matrix_test.repeat(X_test_tensor.shape[0], 1, 1).to(device)\n",
        "\n",
        "# Creating an instance of the custom Dataset\n",
        "dataset = MyDataset(X_train_tensor, y_train_tensor, adj_matrix_np)\n",
        "\n",
        "# Creating the DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 50\n",
        "best_valid_loss = float('inf')  # Initialize with a large value\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels, adjs in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        adjs = adjs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, adjs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    # valid_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "            outputs = model(X_test_tensor, adj_matrix_test)\n",
        "            valid_loss = criterion(outputs, y_test_tensor)\n",
        "    #         valid_loss += loss.item()\n",
        "\n",
        "    # valid_loss /= len(valid_dataloader)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the model checkpoint\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pt')\n",
        "        print(\"Best model checkpoint saved!\")\n",
        "\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# if best_model_state_dict is not None:\n",
        "model.load_state_dict(torch.load(\"best_model_checkpoint.pt\"))\n",
        "print(\"Best model state dictionary loaded!\")"
      ],
      "metadata": {
        "id": "ofhHtMeIufn9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}